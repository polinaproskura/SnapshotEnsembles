{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torchvision.models as models\n",
    "import torch, torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def transform(my_x, my_y):\n",
    "    tensor_x = torch.Tensor(my_x) # transform to torch tensor\n",
    "    tensor_y = torch.LongTensor(my_y)\n",
    "\n",
    "    my_dataset = data.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "    my_dataloader = data.DataLoader(my_dataset) # create your dataloader\n",
    "    return my_dataloader\n",
    "    \n",
    "def compute_loss(X_batch, y_batch):\n",
    "    X_batch = Variable(torch.FloatTensor(X_batch))\n",
    "    y_batch = Variable(torch.LongTensor(y_batch))\n",
    "    #X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "    logits = resnet18(X_batch)\n",
    "    return F.cross_entropy(logits, y_batch).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "batch_size=100\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(28, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = datasets.FashionMNIST('fashion-10', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST('fashion-10', download=True, train=False, transform=transform)\n",
    "print(trainset[0][0].shape)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_lr(stepsize, min_lr=3e-4, max_lr=3e-3):\n",
    "\n",
    "    # Scaler: we can adapt this if we do not want the triangular CLR\n",
    "    scaler = lambda x: 1. #* math.exp(-x * 0.1)\n",
    "\n",
    "    # Lambda function to calculate the LR\n",
    "    lr_lambda = lambda it: min_lr + (max_lr - min_lr) * relative(it, stepsize)\n",
    "\n",
    "    # Additional function to see where on the cycle we are\n",
    "    def relative(it, stepsize):\n",
    "        cycle = math.floor(1 + it / (2 * stepsize))\n",
    "        x = abs(it / stepsize - 2 * cycle + 1)\n",
    "        return max(0, (1 - x)) * scaler(cycle)\n",
    "\n",
    "    return lr_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = ResNet(BasicBlock, [2,2,2,2])\n",
    "#resnet18 = resnet18.cuda()\n",
    "\n",
    "num_epochs = 21\n",
    "start_lr = 0.01\n",
    "end_lr = 0.1\n",
    "factor = 100\n",
    "opt = torch.optim.Adam(resnet18.parameters(), lr=end_lr)\n",
    "\n",
    "step_size = 4*len(trainset) / batch_size\n",
    "clr = cyclical_lr(step_size, min_lr=start_lr, max_lr=end_lr)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(opt, [clr])\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy = []\n",
    "losses = []\n",
    "\n",
    "lr_find_loss = []\n",
    "learn_rates = []\n",
    "\n",
    "it = 0\n",
    "\n",
    "smoothing = 0.5\n",
    "\n",
    "current_accuracies = []\n",
    "current_models = []\n",
    "rolling_means = []\n",
    "models_amount = 5\n",
    "rolling_len = 10\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    k = 0\n",
    "    resnet18.train(True)\n",
    "    for (X_batch, y_batch) in trainloader:\n",
    "        loss = compute_loss(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss.append(loss.data.cpu().numpy())\n",
    "        k += 1\n",
    "\n",
    "        scheduler.step()\n",
    "        opt.zero_grad()\n",
    "        lr_step = opt.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "        learn_rates.append(lr_step)\n",
    "\n",
    "        lr_find_loss.append(loss)\n",
    "        it += 1\n",
    "    \n",
    "    resnet18.train(False)\n",
    "    for (X_batch, y_batch) in testloader:\n",
    "        #y_batch = y_batch.cuda()\n",
    "        logits = resnet18(Variable(torch.FloatTensor(X_batch)))#.cuda())\n",
    "        y_pred = logits.max(1)[1].data\n",
    "        val_accuracy.append(np.mean((y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "    \n",
    "    current_accuracies.append(val_accuracy[-1])\n",
    "    if len(current_accuracies) >= 10:\n",
    "        rolling_means.append(np.mean(np.array(current_accuracies[-10:])))\n",
    "        current_models.append(copy.deepcopy(resnet18))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "          epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "          np.mean(train_loss[-len(trainset) // batch_size :])))\n",
    "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "          np.mean(val_accuracy[-len(testset) // batch_size :]) * 100))\n",
    "    losses.append(np.array(train_loss).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(train_loss))), train_loss)\n",
    "plt.plot(list(range(len(learn_rates))), (np.array(learn_rates) * 1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(current_accuracies))), current_accuracies)\n",
    "plt.plot(list(range(len(learn_rates))), (np.array(learn_rates) * 1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_five = choose_five_max_ind(rolling_means)\n",
    "models_for_evaluation = current_models[top_five]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def ensemble_predictions(members, loader):\n",
    "    accs = []\n",
    "    for (X_batch, y_batch) in loader:\n",
    "        yhats = []\n",
    "        for model in members:\n",
    "            #y_batch = y_batch.cuda()\n",
    "            logits = model(Variable(torch.FloatTensor(X_batch)))#.cuda())\n",
    "            y_pred = logits.max(1)[1].data\n",
    "            yhats.append(y_pred.cpu().numpy())\n",
    "        yhats = np.array(yhats)\n",
    "        maxs = []\n",
    "        for i in range(yhats.shape[1]):\n",
    "            amounts = Counter(yhats[:, i])\n",
    "            most_freq = amounts.most_common(1)\n",
    "            maxs.append(most_freq[0][0])\n",
    "        # sum across ensemble members\n",
    "        maxs = np.array(maxs)\n",
    "        # argmax across classes\n",
    "        accs.append(np.mean((y_batch.cpu().numpy() == maxs)))\n",
    "    accs = np.array(accs)\n",
    "    return accs, np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs, mean_accs = ensemble_predictions(models_for_evaluation[3:], testloader)\n",
    "print(round(mean_accs, 3) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
