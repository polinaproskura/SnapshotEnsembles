{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet_temperature.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "VdSsjRBKkjCj"
      },
      "source": [
        "#@title imports\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "import time\n",
        "import copy\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torchvision.models as models\n",
        "import torch, torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "def transform(my_x, my_y):\n",
        "    tensor_x = torch.Tensor(my_x)  # transform to torch tensor\n",
        "    tensor_y = torch.LongTensor(my_y)\n",
        "    my_dataset = data.TensorDataset(tensor_x, tensor_y)  # create your datset\n",
        "    my_dataloader = data.DataLoader(my_dataset)  # create your dataloader\n",
        "    return my_dataloader\n",
        "\n",
        "\n",
        "def compute_loss(X_batch, y_batch):\n",
        "    X_batch = Variable(torch.FloatTensor(X_batch))\n",
        "    y_batch = Variable(torch.LongTensor(y_batch))\n",
        "    X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
        "    logits = resnet18(X_batch)\n",
        "    return F.cross_entropy(logits, y_batch).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "9X3NmX5GkoKl",
        "outputId": "84cd6802-9b40-42ab-c7ce-9007fd5f6e06"
      },
      "source": [
        "#@title dataloading\n",
        "transform = transforms.ToTensor()\n",
        "batch_size=100\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(28, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = datasets.CIFAR10('cifar-10', download=True, train=True, transform=transform)\n",
        "testset = datasets.CIFAR10('cifar-10', download=True, train=False, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "valset, testset = torch.utils.data.random_split(testset, [100, 9900])\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Btl5i05lkqps"
      },
      "source": [
        "#@title resnet_structure\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srkJEP9pkt_n"
      },
      "source": [
        "def cyclical_lr(stepsize, min_lr=3e-4, max_lr=3e-3):\n",
        "\n",
        "    # Scaler: we can adapt this if we do not want the triangular CLR\n",
        "    scaler = lambda x: 1. #* math.exp(-x * 0.1)\n",
        "\n",
        "    # Lambda function to calculate the LR\n",
        "    lr_lambda = lambda it: min_lr + (max_lr - min_lr) * relative(it, stepsize)\n",
        "\n",
        "    # Additional function to see where on the cycle we are\n",
        "    def relative(it, stepsize):\n",
        "        cycle = math.floor(1 + it / (2 * stepsize))\n",
        "        x = abs(it / stepsize - 2 * cycle + 1)\n",
        "        return max(0, (1 - x)) * scaler(cycle)\n",
        "\n",
        "    return lr_lambda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW9GptFqlfh3",
        "outputId": "cc043afd-64b8-41e5-d16f-27a9e2e8a13e"
      },
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "resnet18 = ResNet(BasicBlock, [2,2,2,2])\n",
        "resnet18 = resnet18.cuda()\n",
        "models_list = []\n",
        "prev_model = copy.deepcopy(resnet18)\n",
        "prev_loss = 1000.\n",
        "\n",
        "num_epochs = 351\n",
        "start_lr = 0.01\n",
        "end_lr = 0.1\n",
        "factor = 10\n",
        "\n",
        "opt = torch.optim.SGD(resnet18.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "step_size = 1*len(trainset) / batch_size\n",
        "print(step_size)\n",
        "clr = cyclical_lr(step_size, min_lr=start_lr, max_lr=end_lr)\n",
        "#scheduler = torch.optim.lr_scheduler.LambdaLR(opt, [clr])\n",
        "\n",
        "train_loss = []\n",
        "val_accuracy = []\n",
        "mean_losses = []\n",
        "mean_accs = []\n",
        "models_list = []\n",
        "\n",
        "lr_find_loss = []\n",
        "learning_rates = []\n",
        "likely_loss = []\n",
        "losses = []\n",
        "\n",
        "smoothing = 0.5\n",
        "lrs = []\n",
        "\n",
        "it = 0\n",
        "\n",
        "smoothing = 0.5\n",
        "small_flag = False\n",
        "#scheduler = torch.optim.lr_scheduler.LambdaLR(opt, [clr])\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    resnet18.train(True)\n",
        "    for (X_batch, y_batch) in trainloader:\n",
        "        loss = compute_loss(X_batch, y_batch)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        num_loss = loss.data.cpu().numpy()\n",
        "        train_loss.append(num_loss)\n",
        "        if epoch <= 150:\n",
        "            opt.step()\n",
        "        else:\n",
        "            scheduler.step()\n",
        "        opt.zero_grad()\n",
        "        lr_step = opt.state_dict()[\"param_groups\"][0][\"lr\"]\n",
        "        if lr_step < 0.001036:\n",
        "            models_list.append(copy.deepcopy(resnet18))\n",
        "            likely_loss.append(num_loss)\n",
        "            if len(models_list) > 25:\n",
        "                models_list = models_list[-25:]\n",
        "        learning_rates.append(lr_step)\n",
        "\n",
        "    if epoch == 150:\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(opt, [clr])  \n",
        "\n",
        "    resnet18.train(False)\n",
        "    for (X_batch, y_batch) in testloader:\n",
        "        y_batch = y_batch.cuda()\n",
        "        logits = resnet18(Variable(torch.FloatTensor(X_batch)).cuda())\n",
        "        y_pred = logits.max(1)[1].data\n",
        "        val_accuracy.append(np.mean((y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
        "    \n",
        "\n",
        "    mean_accs.append(np.mean(val_accuracy[-len(testset) // batch_size:]))\n",
        "    cur_loss = np.mean(train_loss[-len(trainset) // batch_size:])\n",
        "    mean_losses.append(cur_loss)\n",
        "    \n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "            epoch + 1, num_epochs, time.time() - start_time))\n",
        "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
        "            np.mean(train_loss[-len(trainset) // batch_size :])))\n",
        "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
        "            np.mean(val_accuracy[-len(testset) // batch_size :]) * 100))\n",
        "    losses.append(np.array(train_loss).mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500.0\n",
            "Epoch 1 of 351 took 73.119s\n",
            "  training loss (in-iteration): \t2.208253\n",
            "  validation accuracy: \t\t\t24.67 %\n",
            "Epoch 11 of 351 took 72.720s\n",
            "  training loss (in-iteration): \t0.553525\n",
            "  validation accuracy: \t\t\t66.82 %\n",
            "Epoch 21 of 351 took 72.425s\n",
            "  training loss (in-iteration): \t0.483124\n",
            "  validation accuracy: \t\t\t68.02 %\n",
            "Epoch 31 of 351 took 72.081s\n",
            "  training loss (in-iteration): \t0.467601\n",
            "  validation accuracy: \t\t\t76.62 %\n",
            "Epoch 41 of 351 took 71.931s\n",
            "  training loss (in-iteration): \t0.448886\n",
            "  validation accuracy: \t\t\t71.81 %\n",
            "Epoch 51 of 351 took 71.995s\n",
            "  training loss (in-iteration): \t0.442036\n",
            "  validation accuracy: \t\t\t71.64 %\n",
            "Epoch 61 of 351 took 71.889s\n",
            "  training loss (in-iteration): \t0.432482\n",
            "  validation accuracy: \t\t\t75.49 %\n",
            "Epoch 71 of 351 took 71.778s\n",
            "  training loss (in-iteration): \t0.422699\n",
            "  validation accuracy: \t\t\t69.77 %\n",
            "Epoch 81 of 351 took 71.647s\n",
            "  training loss (in-iteration): \t0.435560\n",
            "  validation accuracy: \t\t\t70.59 %\n",
            "Epoch 91 of 351 took 71.551s\n",
            "  training loss (in-iteration): \t0.437668\n",
            "  validation accuracy: \t\t\t68.67 %\n",
            "Epoch 101 of 351 took 71.577s\n",
            "  training loss (in-iteration): \t0.437841\n",
            "  validation accuracy: \t\t\t66.85 %\n",
            "Epoch 111 of 351 took 71.601s\n",
            "  training loss (in-iteration): \t0.428752\n",
            "  validation accuracy: \t\t\t74.63 %\n",
            "Epoch 121 of 351 took 71.400s\n",
            "  training loss (in-iteration): \t0.430895\n",
            "  validation accuracy: \t\t\t73.98 %\n",
            "Epoch 131 of 351 took 71.423s\n",
            "  training loss (in-iteration): \t0.435102\n",
            "  validation accuracy: \t\t\t77.96 %\n",
            "Epoch 141 of 351 took 71.277s\n",
            "  training loss (in-iteration): \t0.421750\n",
            "  validation accuracy: \t\t\t66.23 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqUTAxozoMN_"
      },
      "source": [
        "original_likely_loss = likely_loss.copy() # only for train\n",
        "print(original_likely_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oaGb7Zat82F"
      },
      "source": [
        "def weigheted_most_common(predictions, weights):\n",
        "    new_predictions = {}\n",
        "    for i in range(len(predictions)):\n",
        "        if predictions[i] in new_predictions:\n",
        "            new_predictions[predictions[i]] = new_predictions[predictions[i]] + weights[i]\n",
        "        else:\n",
        "            new_predictions[predictions[i]] = weights[i]\n",
        "    top = list(sorted(new_predictions.items(), key=lambda item: item[1], reverse=True))\n",
        "    return top[0][0]\n",
        "\n",
        "    \n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "def ensemble_predictions(members, loader, weights):\n",
        "    accs = []\n",
        "    for (X_batch, y_batch) in loader:\n",
        "        yhats = []\n",
        "        for model in members:\n",
        "            y_batch = y_batch.cuda()\n",
        "            logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
        "            y_pred = logits.max(1)[1].data\n",
        "            yhats.append(y_pred.cpu().numpy())\n",
        "        yhats = np.array(yhats)\n",
        "        maxs = []\n",
        "        for i in range(yhats.shape[1]):\n",
        "            cur_max = weigheted_most_common(yhats[:, i], weights)\n",
        "            maxs.append(cur_max)\n",
        "        # sum across ensemble members\n",
        "        maxs = np.array(maxs)\n",
        "        # argmax across classes\n",
        "        accs.append(np.mean((y_batch.cpu().numpy() == maxs)))\n",
        "    accs = np.array(accs)\n",
        "    return accs, np.mean(accs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64wCl_LbuC6i"
      },
      "source": [
        "temps = [0.1, 0.3, 0.5, 0.8, 1.1, 1.5,]\n",
        "ens_accs = []\n",
        "lik_loss = original_likely_loss.copy()\n",
        "print(len(models_list))\n",
        "for t in temps:\n",
        "    temp_constant = t\n",
        "    loss_list = np.array(lik_loss[-len(models_list):])\n",
        "    for i in range(loss_list.shape[0]):\n",
        "        loss_list[i] = np.exp(loss_list[i] / temp_constant)\n",
        "    weights = np.array(1. / loss_list, dtype=np.float64) #weight according likelyhood \n",
        "    #print(weights)\n",
        "    accs, mean_accs = ensemble_predictions(models_list, testloader, weights)\n",
        "    ens_accs.append(mean_accs)\n",
        "    print(t, 'Ensemble accuracy = ', round(mean_accs, 3) * 100, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yjaoVEpuMTx"
      },
      "source": [
        "plt.plot(temps, ens_accs)\n",
        "plt.title('Weights according train loss')\n",
        "plt.xlabel('temperature')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co0x8cG5ukU1"
      },
      "source": [
        "#aacuracy of every one\n",
        "for model in models_list:\n",
        "  accuracy = []\n",
        "  for (X_batch, y_batch) in testloader:\n",
        "          y_batch = y_batch.cuda()\n",
        "          logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
        "          y_pred = logits.max(1)[1].data\n",
        "          accuracy.append(np.mean((y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
        "  print(np.array(accuracy).mean() * 100, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPwLLbjrukTh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r94lqWlEukSL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}